import torch
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    
#===========================#
# è¼‰å…¥ä¸¦è™•ç†æ–‡æœ¬è³‡æ–™
#===========================#
def load_and_split_documents(filepath: str, chunk_size=500, chunk_overlap=50):
    loader = TextLoader(filepath, encoding="utf-8")
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_documents(documents)

#===========================#
# å»ºç«‹å‘é‡è³‡æ–™åº«
#===========================#
def create_vector_db(splits, db_model_path):
    embedding = HuggingFaceEmbeddings(
        model_name=db_model_path,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )
    return FAISS.from_documents(splits, embedding)

#===========================#
# æŸ¥è©¢ç›¸ä¼¼å…§å®¹
#===========================#
def retrieve_context(db, query, k=1):
    docs = db.similarity_search(query, k=k)
    for i, doc in enumerate(docs):
        print(f"[åŒ¹é…æ®µè½ {i+1}]\n{doc.page_content}\n{'='*30}")
    return "\n".join([doc.page_content for doc in docs])

#===========================#
# å»ºæ§‹æç¤ºèª (Prompt)
#===========================#
def build_prompt(context: str, query: str):
    return f"""
è«‹ç”¨æ¢åˆ—å¼å›ç­”ï¼Œä¸¦ç¢ºä¿èªå¥å®Œæ•´ã€‚

### å…§å®¹:
{context}
### å•é¡Œ:
{query}
### LLMçš„å›æ‡‰:"""

#===========================#
# è¼‰å…¥æœ¬åœ°é‡åŒ–æ¨¡å‹
#===========================#
def load_local_model(model_id: str):
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    return tokenizer, model

#===========================#
# åŸ·è¡Œæ¨ç†ä¸¦è¼¸å‡º
#===========================#
def generate_answer(tokenizer, model, prompt: str, max_new_tokens=500):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_len = inputs['input_ids'].shape[-1]

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        top_k=5,
        num_return_sequences=1
    )

    for i, output in enumerate(outputs):
        generated_tokens = output[input_len:]
        answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        print(f"[å›ç­” {i+1}]\n{answer}\n{'-'*40}")

#===========================#
# ä¸»åŸ·è¡Œæµç¨‹
#===========================#
def run_rag_pipeline(model_path: str, db_model_path: str, data_path: str, user_query: str):
    tokenizer, model = load_local_model(model_path)  # è¼‰å…¥æœ¬åœ°é‡åŒ–æ¨¡å‹
    splits = load_and_split_documents(data_path)  # è¼‰å…¥ä¸¦è™•ç†æ–‡æœ¬è³‡æ–™
    db = create_vector_db(splits, db_model_path) # å»ºç«‹å‘é‡è³‡æ–™åº«
    context = retrieve_context(db, user_query)  # æŸ¥è©¢ç›¸ä¼¼å…§å®¹
    # context = ""  # ä¸ä½¿ç”¨RAG
    prompt = build_prompt(context, user_query)  # å»ºæ§‹æç¤ºèª (Prompt)
    generate_answer(tokenizer, model, prompt)  # åŸ·è¡Œæ¨ç†ä¸¦è¼¸å‡º

#===========================#
# åŸ·è¡Œç¯„ä¾‹
#===========================#
if __name__ == "__main__":
    import os
    base_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(base_dir, "..", ".."))
    model_path = os.path.join(project_root, "models", "taide--Llama-3.1-TAIDE-LX-8B-Chat")
    db_model_path = os.path.join(project_root, "models", "shibing624--text2vec-base-chinese")

    # data_path = os.path.join(project_root, "data", "merged_en.txt")
    data_path = os.path.join(project_root, "data", "merged_ch.txt")
    question = "æ¨è–¦æˆ‘ä¸€å°å¤–é€ç„¡äººæ©Ÿ"

    # data_path = os.path.join(project_root, "data", "demo.txt")
    # question = "2025ç©ºé›£äº‹ä»¶"

    print(f"ğŸ“Œ å•é¡Œï¼š{ question}\n")
    run_rag_pipeline(model_path, db_model_path, data_path, question)
